# 正则化和训练频率优化说明

## 一、修改内容

### 1. 训练频率优化

**文件**: `config/pipeline.yaml`

**修改**:
```yaml
rolling:
  train_months: 12  # 从48个月缩短到12个月
  step_months: 3    # 每季度（3个月）重新训练一次
```

**原因**:
- 原配置使用48个月（4年）的训练数据，包含过多历史信息
- 市场环境变化快，长期历史数据可能不再适用
- 缩短到12个月可以提高模型对近期市场的适应性
- 每季度重新训练可以在模型时效性和训练成本之间取得平衡
  - 季度训练频率足够捕捉市场变化
  - 相比每月训练，减少计算成本
  - 相比每年训练，保持模型时效性

**效果预期**:
- 提高模型对近期市场变化的敏感性
- 减少历史噪音的影响
- 提高预测IC值
- 在模型时效性和训练成本之间取得平衡

### 2. LightGBM 正则化增强

**文件**: `config/model_lgb.yaml`

**修改**:
```yaml
params:
  lambda_l1: 0.5      # 从0.2增加到0.5
  lambda_l2: 2.0      # 从1.0增加到2.0
  feature_fraction: 0.7  # 从0.8降到0.7
  bagging_fraction: 0.6  # 从0.7降到0.6
```

**原因**:
- `lambda_l1` 和 `lambda_l2` 控制L1和L2正则化强度
- 增加正则化可以减少过拟合，提高泛化能力
- 降低 `feature_fraction` 和 `bagging_fraction` 增加随机性，也是一种正则化

**效果预期**:
- 减少过拟合
- 提高模型泛化能力
- 提高预测时的IC值

### 3. MLP 正则化增强

**文件**: `config/model_mlp.yaml`

**修改**:
```yaml
dropout: 0.4          # 从0.3增加到0.4
weight_decay: 5e-4    # 从1e-4增加到5e-4
```

**原因**:
- `dropout` 在训练时随机丢弃神经元，防止过拟合
- `weight_decay` 是L2正则化，惩罚大权重
- 增加这两个参数可以提高模型泛化能力

**效果预期**:
- 减少过拟合
- 提高模型稳定性
- 提高预测时的IC值

### 4. Stack 模型正则化增强

**文件**: `config/model_stack.yaml`

**修改**:
```yaml
dropout: 0.6          # 从0.5增加到0.6
weight_decay: 5e-4    # 从1e-4增加到5e-4
```

**原因**:
- Stack模型是二级学习器，更容易过拟合
- 需要更强的正则化来防止过拟合

**效果预期**:
- 减少Stack模型的过拟合
- 提高整体模型的稳定性

## 二、使用说明

### 重新训练模型

修改配置后，需要重新训练模型：

```bash
conda activate qlib_zhengshi
cd D:\lianghuatouzi\Qlib1124\project
python run_train.py --config config/pipeline.yaml
```

### 训练频率说明

**当前配置**: 每季度（3个月）重新训练一次

- **训练窗口**: 使用过去12个月的数据
- **滚动步长**: 每3个月向前滚动一次
- **训练时间点**: 每季度末（3月、6月、9月、12月）
- **示例**: 
  - 2024-Q1: 训练数据 2023-01 到 2023-12，模型用于 2024-01 到 2024-03
  - 2024-Q2: 训练数据 2023-04 到 2024-03，模型用于 2024-04 到 2024-06
  - 2024-Q3: 训练数据 2023-07 到 2024-06，模型用于 2024-07 到 2024-09
  - 2024-Q4: 训练数据 2023-10 到 2024-09，模型用于 2024-10 到 2024-12

**优势**:
- ✅ 在模型时效性和训练成本之间取得平衡
- ✅ 季度训练频率足够捕捉市场变化
- ✅ 相比每月训练，减少计算成本
- ✅ 相比每年训练，保持模型时效性

### 验证改进效果

训练完成后，可以：

1. **检查训练日志中的IC值**:
```bash
python scripts/analyze_training_ic.py
```

2. **生成新的预测**:
```bash
python run_predict.py --tag auto --start 2023-10-01 --end 2025-10-01
```

3. **分析预测质量**:
```bash
python scripts/analyze_prediction_quality.py --prediction data/predictions/pred_<tag>_2023-10-01_2025-10-01.csv
```

4. **运行回测**:
```bash
python run_backtest.py --prediction data/predictions/pred_<tag>_2023-10-01_2025-10-01.csv --use-rqalpha
```

## 三、预期改进

### 1. 训练-预测IC差距缩小

- **当前**: 训练IC 0.06，预测IC 0.026（差距56%）
- **预期**: 训练IC 0.05-0.06，预测IC 0.04-0.05（差距缩小到20%以内）

### 2. IC值稳定性提高

- **当前**: IC标准差 0.14-0.16
- **预期**: IC标准差降低到 0.10-0.12

### 3. 2025年IC值改善

- **当前**: 2025年平均IC 0.02
- **预期**: 2025年平均IC 0.04-0.05

## 四、注意事项

1. **训练时间**: 缩短训练窗口后，每个窗口的训练时间会减少，但总训练次数会增加（因为每月都要训练）

2. **数据量**: 12个月的训练数据可能比48个月少，如果数据量不足，可能需要调整 `min_samples` 参数

3. **正则化强度**: 如果正则化过强，可能导致模型欠拟合。如果发现训练IC值明显下降，可以适当降低正则化参数

4. **监控指标**: 建议同时监控训练IC和预测IC，确保两者差距在合理范围内

## 五、进一步优化建议

如果上述改进效果不明显，可以考虑：

1. **进一步缩短训练窗口**: 从12个月缩短到6-9个月
2. **调整正则化参数**: 根据实际效果微调
3. **特征工程**: 添加更多有效特征或进行特征选择
4. **模型架构**: 尝试不同的模型架构或超参数

