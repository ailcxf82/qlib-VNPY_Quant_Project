# 因子重要性衡量与筛选指南

## 一、因子重要性的衡量方法

### 1.1 LightGBM 的两种重要性指标

LightGBM 提供了两种衡量特征重要性的方法：

#### **Gain（增益）重要性** ⭐ 推荐
- **定义**：特征在所有树中带来的总增益（损失函数减少量）
- **计算方式**：每次使用该特征进行分裂时，计算分裂前后的损失函数差值，累加所有分裂的增益
- **优点**：
  - 直接反映特征对模型预测能力的贡献
  - 考虑了特征的实际预测价值
  - 更准确地识别真正重要的特征
- **适用场景**：**推荐使用**，特别是用于特征筛选

**示例**：
```
特征 A 的 Gain = 1000  (在所有树中带来的总增益)
特征 B 的 Gain = 500   (贡献较小)
→ 特征 A 更重要
```

#### **Split（分裂次数）重要性**
- **定义**：特征在所有树中被用作分裂点的次数
- **计算方式**：统计该特征在多少棵树中被选择进行分裂
- **优点**：
  - 计算简单，易于理解
  - 反映特征被使用的频率
- **缺点**：
  - 不考虑每次分裂的实际贡献
  - 可能高估频繁使用但贡献小的特征
- **适用场景**：了解特征使用频率，但不推荐用于筛选

**示例**：
```
特征 A 被使用了 50 次，但每次增益很小
特征 B 被使用了 20 次，但每次增益很大
→ Split 重要性：A > B
→ Gain 重要性：B > A（更准确）
```

### 1.2 其他重要性衡量方法（可选）

除了 LightGBM 内置的重要性，还可以使用：

1. **Permutation Importance（排列重要性）**
   - 随机打乱特征值，观察模型性能下降
   - 性能下降越大，特征越重要
   - 更稳健，但计算成本高

2. **SHAP 值**
   - 基于博弈论的特征贡献度
   - 可以解释每个样本的特征贡献
   - 计算成本较高

3. **相关性分析**
   - 计算特征与标签的相关系数
   - 简单快速，但只能捕捉线性关系

## 二、为什么要筛选因子？

### 2.1 问题：因子数量过多

- **Alpha158 因子**：约 158 个
- **自定义因子**：可能还有几十到上百个
- **总计**：可能超过 200 个因子

### 2.2 因子过多的负面影响

1. **维度诅咒（Curse of Dimensionality）**
   - 高维空间中数据稀疏
   - 模型难以学习有效模式
   - 容易过拟合

2. **噪声因子干扰**
   - 很多因子可能对预测没有帮助
   - 噪声因子会干扰模型学习
   - 降低模型泛化能力

3. **计算资源浪费**
   - 特征计算成本高
   - 模型训练时间长
   - 存储和推理成本增加

4. **模型可解释性差**
   - 因子太多难以理解
   - 难以识别关键驱动因子

### 2.3 筛选因子的好处

✅ **提升模型性能**：去除噪声，聚焦重要因子  
✅ **降低过拟合风险**：减少模型复杂度  
✅ **加快训练速度**：减少特征计算和模型训练时间  
✅ **提高可解释性**：识别关键驱动因子  
✅ **降低维护成本**：减少需要维护的因子数量  

## 三、筛选策略：是否只保留重要性高的因子？

### 3.1 基本策略：保留重要性高的因子 ✅

**是的，通常只保留重要性高的因子。**

**原因**：
- 重要性低的因子对预测贡献小，可能是噪声
- 保留过多低重要性因子会增加模型复杂度
- 聚焦高重要性因子能提升模型性能

### 3.2 筛选方法

#### 方法 1：Top-K 筛选（推荐）
```python
# 保留重要性前 75 个因子
top_k = 75
top_features = importance_df.head(top_k)["feature_name"].tolist()
```

**优点**：
- 简单直接
- 控制因子数量
- 适合资源有限的情况

**缺点**：
- 可能忽略某些重要但排名靠后的因子
- 需要根据经验选择 K 值

#### 方法 2：阈值筛选
```python
# 保留重要性超过阈值的因子
min_importance = 0.001
filtered_features = importance_df[
    importance_df["importance_mean"] >= min_importance
]["feature_name"].tolist()
```

**优点**：
- 基于绝对重要性值
- 自动确定因子数量

**缺点**：
- 阈值难以确定
- 可能保留过多或过少因子

#### 方法 3：组合策略（推荐）⭐
```python
# 结合 Top-K 和阈值
top_k = 75
min_importance = 0.001

# 先按阈值过滤
filtered = importance_df[importance_df["importance_mean"] >= min_importance]

# 再取前 K 个
top_features = filtered.head(top_k)["feature_name"].tolist()
```

**优点**：
- 兼顾数量控制和质量保证
- 更稳健的筛选结果

### 3.3 多模型聚合的重要性

**为什么需要聚合多个滚动窗口模型的重要性？**

1. **时间稳定性**：单个模型可能受特定时间窗口影响
2. **稳健性**：多个模型的平均值更可靠
3. **一致性**：识别在不同时期都重要的因子

**聚合方法**：
```python
# 计算多个模型的重要性统计
importance_mean = df.mean(axis=1)      # 平均重要性
importance_std = df.std(axis=1)        # 标准差（稳定性）
importance_max = df.max(axis=1)        # 最大重要性
model_count = df.count(axis=1)          # 出现次数
```

**筛选建议**：
- 优先选择 `importance_mean` 高的因子
- 考虑 `importance_std` 小的因子（更稳定）
- 确保 `model_count` 足够（在多个窗口都出现）

## 四、实际应用建议

### 4.1 筛选流程

```
1. 训练多个滚动窗口模型
   ↓
2. 提取每个模型的特征重要性（使用 Gain）
   ↓
3. 聚合所有模型的重要性（计算均值、标准差等）
   ↓
4. 按平均重要性排序
   ↓
5. 筛选前 50-100 个因子（或设置阈值）
   ↓
6. 验证筛选后的模型性能
   ↓
7. 根据验证结果调整筛选策略
```

### 4.2 筛选数量建议

| 场景 | 因子数量 | 说明 |
|------|---------|------|
| **资源充足** | 80-100 个 | 保留更多因子，捕捉更多信息 |
| **标准配置** | 50-75 个 | 平衡性能和效率 ⭐ 推荐 |
| **资源有限** | 30-50 个 | 只保留核心重要因子 |
| **快速实验** | 20-30 个 | 快速验证想法 |

### 4.3 验证筛选效果

筛选后需要验证：

1. **模型性能对比**
   ```python
   # 使用全部因子训练
   model_all = train_model(features_all)
   ic_all = evaluate(model_all)
   
   # 使用筛选后的因子训练
   model_filtered = train_model(features_filtered)
   ic_filtered = evaluate(model_filtered)
   
   # 对比 IC 值
   print(f"全部因子 IC: {ic_all:.4f}")
   print(f"筛选后 IC: {ic_filtered:.4f}")
   ```

2. **性能指标**
   - **IC（信息系数）**：应该保持或提升
   - **IC-IR（IC 信息比率）**：应该提升（更稳定）
   - **回测收益**：应该保持或提升

3. **如果性能下降**
   - 增加筛选的因子数量（提高 top_k）
   - 降低最小重要性阈值
   - 检查是否误删了重要因子

### 4.4 因子分类筛选（可选）

可以按因子类型分别筛选，确保各类因子都有代表：

```python
# 按类型筛选
kbar_features = filter_by_category(features, "kbar", top_k=9)
price_features = filter_by_category(features, "price", top_k=4)
rolling_features = filter_by_category(features, "rolling", top_k=60)
custom_features = filter_by_category(features, "custom", top_k=10)
```

**优点**：
- 保证因子多样性
- 避免某类因子被完全过滤

**缺点**：
- 可能保留一些重要性较低的因子
- 需要手动设置每类的数量

## 五、使用示例

### 5.1 基本使用

```bash
# 筛选前 75 个重要因子（使用 Gain 重要性）
python scripts/analyze_feature_importance.py \
    --config config/pipeline.yaml \
    --top_k 75 \
    --importance_type gain
```

### 5.2 结合阈值筛选

```bash
# 筛选前 100 个因子，且重要性 >= 0.001
python scripts/analyze_feature_importance.py \
    --config config/pipeline.yaml \
    --top_k 100 \
    --min_importance 0.001 \
    --importance_type gain
```

### 5.3 查看结果

```python
import pandas as pd

# 查看所有特征的重要性
df = pd.read_csv("data/logs/feature_importance.csv")
print(df.head(20))

# 查看筛选后的特征列表
with open("data/logs/top_features.txt", "r") as f:
    top_features = [line.strip() for line in f]
print(f"筛选出 {len(top_features)} 个重要因子")
```

## 六、常见问题

### Q1: 是否应该完全删除低重要性因子？

**A**: 通常是的，但要注意：
- 如果某个因子在某个时期很重要，但在平均重要性中排名靠后，可能需要保留
- 可以设置一个较低的阈值（如 `min_importance=0.0001`）作为兜底

### Q2: 重要性为 0 的因子是否应该删除？

**A**: 是的，重要性为 0 表示该因子从未被使用，可以安全删除。

### Q3: 如何确定 top_k 的值？

**A**: 
1. 从较大的值开始（如 100）
2. 逐步减少，观察模型性能
3. 选择性能开始明显下降前的值
4. 通常 50-75 是一个好的起点

### Q4: Gain 和 Split 应该用哪个？

**A**: **推荐使用 Gain**，因为它更准确地反映特征的预测价值。

### Q5: 筛选后模型性能下降怎么办？

**A**: 
1. 增加 top_k 值
2. 降低 min_importance 阈值
3. 检查是否有重要因子被误删
4. 考虑按类型筛选，保证因子多样性

## 七、总结

✅ **衡量方法**：使用 **Gain 重要性**（推荐）或 Split 重要性  
✅ **筛选策略**：**只保留重要性高的因子**（Top-K + 阈值组合）  
✅ **筛选数量**：**50-75 个因子**是较好的平衡点  
✅ **验证方法**：对比筛选前后的 IC、IC-IR、回测收益  
✅ **多模型聚合**：聚合多个滚动窗口模型的重要性，提高稳健性  

通过合理的因子筛选，可以在保持或提升模型性能的同时，降低模型复杂度，提高训练效率和可解释性。


